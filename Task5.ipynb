{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8HvmO5mlopy",
        "outputId": "a4ae197a-a305-4995-e8d5-12ae43bce665"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYwz6XLxl7yF"
      },
      "source": [
        "import os\n",
        "os.chdir('drive/MyDrive/Colab/Task5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVcWTBZMZca0"
      },
      "source": [
        "## 1. 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bqznbq5mEN8"
      },
      "source": [
        "def readFile(path):\n",
        "    with open(path) as fp:\n",
        "        datas = []\n",
        "        lines = fp.readlines()\n",
        "\n",
        "        curr_doc = \"\"\n",
        "        for line in lines:\n",
        "            line = line.strip('\\n')\n",
        "\n",
        "            if len(line) != 0:\n",
        "                datas.append(line)\n",
        "\n",
        "    return datas "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIRDWBMOnCpt"
      },
      "source": [
        "train_datas = readFile('dataset/poetryFromTang.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPX3EPPerRIT"
      },
      "source": [
        "import torch.utils.data\n",
        "import jieba\n",
        "\n",
        "class TangPoetryDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, datas):\n",
        "        super(TangPoetryDataset).__init__()\n",
        "        self.vocab, self.idx2word, self.datas = self._tokenize(datas)\n",
        "\n",
        "    def _tokenize(self, datas):\n",
        "        idx2word = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "        word2idx = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
        "        result = []\n",
        "\n",
        "        cur_idx = 4\n",
        "        for line in datas:\n",
        "            line = jieba.lcut(line)\n",
        "            cur_doc = []\n",
        "\n",
        "            for word in line:\n",
        "                # 如果当前词不在词表里，添加进去\n",
        "                if word not in word2idx:\n",
        "                    idx2word.append(word)\n",
        "                    word2idx[word] = cur_idx\n",
        "                    cur_idx = cur_idx + 1\n",
        "                if word != \"，\" and word != \"。\":   # 这个会导致模型学到的只有符号\n",
        "                    cur_doc.append(word2idx[word])\n",
        "            result.append(cur_doc)\n",
        "        return word2idx, idx2word, result\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        doc = [self.vocab['<sos>']] + self.datas[index]\n",
        "        label = self.datas[index] + [self.vocab['<eos>']]\n",
        "        return doc, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUK-fsA_ugJg",
        "outputId": "aae92d7c-d355-4734-c182-59643f76c271"
      },
      "source": [
        "train_dataset = TangPoetryDataset(train_datas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.932 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N34OeXMZgIM"
      },
      "source": [
        "## 2. LSTM模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpxuGc_Pi3Za"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, args, vocab):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.args = args\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=len(vocab),\n",
        "                                      embedding_dim=args.embed_size,\n",
        "                                      padding_idx=0)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=args.embed_size,\n",
        "                            hidden_size=args.hidden_size,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.output = nn.Linear(args.hidden_size, len(vocab))\n",
        "\n",
        "        self.dropout = nn.Dropout(args.dropout_rate)\n",
        "    \n",
        "    def forward(self, doc, mask):\n",
        "        batch_size = doc.shape[0]\n",
        "        # Embedding Layer\n",
        "        # (batch_size, length) -> (batch_size, length, embed_dim)\n",
        "        embed_doc = self.embedding(doc)\n",
        "        embed_doc = self.dropout(embed_doc)\n",
        "\n",
        "        # Input Encoding Layer\n",
        "        # (batch_size, length, embed_dim) -> (batch_size, length, hidden_dim)\n",
        "        encoded_doc, _ = self.lstm(embed_doc)\n",
        "        encoded_doc = self.dropout(encoded_doc)\n",
        "        #encoded_doc = encoded_doc * mask.view(batch_size, -1, 1)\n",
        "\n",
        "        # Output Layer\n",
        "        # (batch_size, length, hidden_dim) -> (batch_size, length, vocab_size)\n",
        "        out = self.output(encoded_doc)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    # 生成文本\n",
        "    def generate(self, seq_in):\n",
        "        len = seq_in.shape[0] # 记录序列长度，防止死循环\n",
        "        \n",
        "        # 处理初始输入序列\n",
        "        seq_in = seq_in.view(1, -1)\n",
        "        embed_seq = self.embedding(seq_in)\n",
        "        _, (h_n, c_n) = self.lstm(embed_seq)\n",
        "        next_token = torch.argmax(self.output(h_n.view(1, -1)).view(-1))\n",
        "\n",
        "        res = torch.cat([seq_in.view(-1), next_token.view(-1)], dim=0)\n",
        "        while next_token != self.vocab['<eos>'] and res.shape[0] < self.args.max_len:\n",
        "            embed_token = self.embedding(next_token.view(1, -1))\n",
        "            _, (h_n, c_n) = self.lstm(embed_token, (h_n, c_n))\n",
        "            next_token = torch.argmax(self.output(h_n.view(1, -1)).view(-1))\n",
        "\n",
        "            res = torch.cat([res.view(-1), next_token.view(-1)], dim=0)\n",
        "\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-YWUki4ZkZh"
      },
      "source": [
        "## 3. 训练过程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt-T9GF41S87"
      },
      "source": [
        "def collate_fn(batch_data):\n",
        "    batch_size = len(batch_data)\n",
        "\n",
        "    # 以batch中最长的句子作为长度进行padding\n",
        "    max_len = max([len(x[0]) for x in batch_data])\n",
        "\n",
        "    vec = torch.ones((batch_size, max_len), dtype=torch.int64)\n",
        "    mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
        "    padded_label = torch.zeros((batch_size, max_len), dtype=torch.int64)\n",
        "    for i, example in enumerate(batch_data):\n",
        "        for j, x in enumerate(example[0]):\n",
        "            vec[i, j] = x\n",
        "            mask[i, j] = 1\n",
        "        \n",
        "        for j, x in enumerate(example[1]):\n",
        "            padded_label[i, j] = x\n",
        "    \n",
        "    return (vec, mask, padded_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIphaYsvX9NA"
      },
      "source": [
        "困惑度计算方法如下：\n",
        "$$\\mathbf{PPL}(\\theta)=(\\Pi_n\\Pi_tp_{\\theta}(x_t^{(n)}|x_{1:(t-1)}^{(n)}))^{-1/T}=\\exp(-\\frac{1}{T}\\Sigma_n\\Sigma_t\\log p_{\\theta}(x_t^{(n)}|x_{1:(t-1)}^{(n)}))=\\exp(-\\frac{1}{T}CE(\\theta))$$\n",
        "根据算式，我们可以通过累加模型的交叉熵，即可算出困惑度。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfv32kTn1e8o"
      },
      "source": [
        "import jieba\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 处理输入序列\n",
        "def preprocess(seq, vocab):\n",
        "    seq = jieba.lcut(seq)\n",
        "    seq = ['<sos>'] + seq\n",
        "\n",
        "    res = torch.zeros((len(seq)), dtype=torch.int64)\n",
        "    for i, x in enumerate(seq):\n",
        "        res[i] = vocab[x]\n",
        "    \n",
        "    return res\n",
        "\n",
        "# 处理输出序列\n",
        "def postprocess(seq, idx2word):\n",
        "    res = \"\"\n",
        "    for x in seq:\n",
        "        res += idx2word[x]\n",
        "    \n",
        "    return res\n",
        "\n",
        "def train_progress(args, model, vocab, idx2word, optimizer, criterion, train_dataloader, device):\n",
        "    train_prep_arr = []\n",
        "    best_train_prep = 1e9\n",
        "    best_epoch = -1\n",
        "    \n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "\n",
        "        # Training\n",
        "        train_len = 0\n",
        "        train_loss = 0.\n",
        "\n",
        "        model.train()\n",
        "        for step, datas in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            text, mask, labels = [data.to(device) for data in datas]\n",
        "        \n",
        "            output = model(text, mask)\n",
        "            output = output.masked_fill(~mask.view(output.shape[0], output.shape[1], 1), 0.).cpu()\n",
        "            loss = criterion(output.view(-1, len(vocab)), labels.view(-1))\n",
        "            \n",
        "            # 训练\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 统计结果\n",
        "            size = labels.numel()\n",
        "            train_len += size\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_prep = np.exp(train_loss / train_len)\n",
        "        train_prep_arr.append(train_prep)\n",
        "        print(f'Train: | perplexity: {train_prep}')\n",
        "\n",
        "        if train_prep < best_train_prep:\n",
        "            best_train_prep = train_prep\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), \"models/best_model.pth\")\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            text = preprocess(\"烟笼寒水月笼沙\", vocab)\n",
        "            res = model.generate(text)\n",
        "            print(postprocess(res, idx2word))\n",
        "    return train_prep_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ1m9Bta7EyA"
      },
      "source": [
        "class Arguments():\n",
        "    epochs = 30\n",
        "    batch_size = 16\n",
        "    lr = 5e-5\n",
        "    embed_size = 50\n",
        "    hidden_size = 50\n",
        "    dropout_rate = 0.2\n",
        "    max_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yDSbXiK7Mmx"
      },
      "source": [
        "import torch.optim\n",
        "\n",
        "args = Arguments()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab = train_dataset.vocab\n",
        "idx2word = train_dataset.idx2word\n",
        "model = LSTM(args, vocab)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMv_nddR7-HE",
        "outputId": "562b14e0-933f-4ef3-fba0-bb42a8b98156"
      },
      "source": [
        "import torch.utils.data\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=args.batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               collate_fn=collate_fn)\n",
        "\n",
        "train_prep_arr = train_progress(args=args,\n",
        "                                model=model,\n",
        "                                vocab=vocab,\n",
        "                                idx2word=idx2word,\n",
        "                                optimizer=optimizer,\n",
        "                                criterion=criterion,\n",
        "                                train_dataloader=train_dataloader, \n",
        "                                device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 27.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5475.2560992542185\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西洋洋新地清栖钓川涨金谷游人霜严玄都齐鲁高标齐鲁高标齐鲁高标暮雨复道彤庭登楼筝长流天生疏驿父执驿父执偃溟驿雷霆朝来盈尺能支毕屠何由竦乱落缘底先贤泪如珠复道之灾之灾\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5461.002788381642\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西有名关西昔时之庆连晓战锒铛三百杯游魂征狄前轩地清栖钓川涨金谷天子再述画图韬当今要津困梁玉绳过射陇亩偃溟驿潇湘潇湘恐泥高标潇湘恐泥再拜字猛将画图清影机巧独送松柏平津\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5449.004625458652\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西有名关西昔时之庆连晓战锒铛三百杯游魂征狄前轩地清栖钓川涨金谷天子再述画图韬当今要津困梁玉绳过射陇亩偃溟驿潇湘潇湘恐泥枭骜过无时竦连晓战梁未试二十四斥余朱炎赫风寒风寒\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5438.0292835147975\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西有名关西昔时之庆连晓战童子晚前轩地清栖钓川涨金谷三十梁玉觞朱炎赫云间蒲为疏主人碧云天碧云天葵藿倾荷俱物役难甘原韬跣辕门缠黄云驱复道之灾之灾皇谟载衰病泾鼓间紫焰浮深异堆臂\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5428.341789447291\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西有名关西昔时之庆连晓战童子晚前轩地清栖钓川涨金谷三十梁玉觞朱炎赫云间蒲为疏主人碧云天碧云天葵藿倾荷肉自僻近世人暮春知君命蒲为愚漂沙献延秋门龙堆连晓战锒铛三百杯钓广文到荷二十\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5413.186902052568\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西有名关西有名涡者夜发画图风寒风寒勤王谢麻姑众山平津臂复道自适臂复道葱葱行自迟萦盈萦盈登楼玉绳缧震之盘右军不宁地清栖钓川涨消息每岁谢麻姑众山平津臂复道自适臂复道葱葱\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5404.079690056019\n",
            "<sos>烟笼寒水月笼沙客中遇天地关西有名关西有名涡者夜发画图风寒风寒勤王谢麻姑众山平津臂复道自适臂复道葱葱行自迟萦盈萦盈登楼玉绳玉泉睢请公问游魂超古今复道赐名炎月北潇湘复道晚玉绳玉泉睢请公问游魂超\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5389.257768885635\n",
            "<sos>烟笼寒水月笼沙客中遇天地既东郊花门复道陇亩偃溟驿潇湘潇湘星复道赐名炎月北潇湘复道晚玉绳玉泉睢请公问游魂超古今复道生鹊广文到登楼走穷谷断绝执秣马地清栖钓川涨金谷游人霜严请公问陌马空天生况资菱下有\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5376.846753648327\n",
            "<sos>烟笼寒水月笼沙客中遇天地既东郊花门复道陇亩偃溟驿潇湘潇湘星鼓间鸿毛以兹悟以兹悟下来自适臂复道自适臂复道葱葱行自迟萦盈萦盈登楼玉绳玉泉睢请公问游魂超古今复道生鹊广文到僻近齐鲁齐鲁高标齐鲁高标\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5363.415258075109\n",
            "<sos>烟笼寒水月笼沙客中遇天地既东郊花门复道行自迟萦盈萦盈天地间竦断折飧炎月北潇湘星复道彤庭登楼筝<eos>\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5358.993387078175\n",
            "<sos>烟笼寒水月笼沙客中遇天地既东郊花门复道湿声云间云间蒲为疏主人芳华西江雷霆竦乱落穷竟慷慨东郊执号谢麻姑众山平津臂复道<eos>\n",
            "Epoch 11:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5341.419271957817\n",
            "<sos>烟笼寒水月笼沙客中遇天地既东郊花门复道<eos>\n",
            "Epoch 12:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5331.2723738296645\n",
            "<sos>烟笼寒水月笼沙客中遇天地既东郊花门复道<eos>\n",
            "Epoch 13:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5316.852334536886\n",
            "<sos>烟笼寒水月笼沙鸿毛鸿毛以兹悟以兹悟下来十年自适亿广文到号自僻近齐鲁名军玉泉<eos>\n",
            "Epoch 14:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5297.928679040219\n",
            "<sos>烟笼寒水月笼沙鸿毛鸿毛以兹悟以兹悟下来十年自适亿<eos>\n",
            "Epoch 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5282.109440343376\n",
            "<sos>烟笼寒水月笼沙鸿毛鸿毛以兹悟以兹悟且且<eos>\n",
            "Epoch 16:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5271.193668368601\n",
            "<sos>烟笼寒水月笼沙鸿毛鸿毛以兹悟<eos>\n",
            "Epoch 17:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5243.691091761153\n",
            "<sos>烟笼寒水月笼沙鸿毛鸿毛以兹悟<eos>\n",
            "Epoch 18:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5222.894398829187\n",
            "<sos>烟笼寒水月笼沙鸿毛鸿毛<eos>\n",
            "Epoch 19:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5200.122809316974\n",
            "<sos>烟笼寒水月笼沙鸿毛<eos>\n",
            "Epoch 20:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5178.585325949592\n",
            "<sos>烟笼寒水月笼沙鸿毛<eos>\n",
            "Epoch 21:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5142.70086431673\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 22:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5108.956158902017\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 23:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 5050.12586627624\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 24:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 4969.272317558007\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 4841.885960578866\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 26:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 4612.426412349055\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 27:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 4321.191964825249\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 28:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 29.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 4091.2394917278593\n",
            "<sos>烟笼寒水月笼沙<eos>\n",
            "Epoch 29:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:01<00:00, 28.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: | perplexity: 3933.4033738074513\n",
            "<sos>烟笼寒水月笼沙<eos>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDllm1gvZpnS"
      },
      "source": [
        "## 4. 结果分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7_KFGzU01I"
      },
      "source": [
        "可能是数据太少或者是我使用了`jieba`提供的中文分词方法，导致词表太少，或者是因为词向量没有经过预训练，导致是词向量质量很差，这样也导致了生成模型训练的效果很差。\\\n",
        "虽然模型到最后能学习到输出以`<eos>`结尾，但纯用贪心搜索的生成方式，模型总喜欢到最后只输出一个`<eos>`，可能需要采用beam search等方式。\\\n",
        "训练时试过将，。这两个符号加上，模型到最后会只生成`，。<eos>`虽然看上去确实学到了符号的关联性，但仅此而已，生成出来的句子完全不能用。"
      ]
    }
  ]
}